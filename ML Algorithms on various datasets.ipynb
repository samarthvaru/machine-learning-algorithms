{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #for manipulating the csv data\n",
    "\n",
    "import numpy as np #for mathematical calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ID3 Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ID3:\n",
    "    def __init__(self):\n",
    "        self.tree={}\n",
    "        \n",
    "    def calc_total_entropy(self,train_data, label, class_list):\n",
    "        total_row = train_data.shape[0]\n",
    "        total_entr = 0\n",
    "\n",
    "        for c in class_list:\n",
    "            total_class_count = train_data[train_data[label] == c].shape[0]\n",
    "            total_class_entr = - (total_class_count/total_row)*np.log2(total_class_count/total_row) \n",
    "            total_entr += total_class_entr\n",
    "\n",
    "        return total_entr\n",
    "    \n",
    "    def calc_entropy(self,feature_value_data, label, class_list):\n",
    "        class_count = feature_value_data.shape[0]\n",
    "        entropy = 0\n",
    "\n",
    "        for c in class_list:\n",
    "            label_class_count = feature_value_data[feature_value_data[label] == c].shape[0]\n",
    "\n",
    "            entropy_class = 0\n",
    "            if label_class_count != 0:\n",
    "                probability_class = label_class_count/class_count\n",
    "                entropy_class = - probability_class * np.log2(probability_class) \n",
    "\n",
    "            entropy += entropy_class\n",
    "\n",
    "        return entropy\n",
    "    \n",
    "    def calc_info_gain(self,feature_name, train_data, label, class_list):\n",
    "        feature_value_list = train_data[feature_name].unique()\n",
    "        total_row = train_data.shape[0]\n",
    "        feature_info = 0.0\n",
    "\n",
    "        for feature_value in feature_value_list:\n",
    "            feature_value_data = train_data[train_data[feature_name] == feature_value]\n",
    "            feature_value_count = feature_value_data.shape[0]\n",
    "            feature_value_entropy = self.calc_entropy(feature_value_data, label, class_list)\n",
    "            feature_value_probability = feature_value_count/total_row\n",
    "            feature_info += feature_value_probability * feature_value_entropy\n",
    "\n",
    "        return self.calc_total_entropy(train_data, label, class_list) - feature_info\n",
    "    \n",
    "    def find_most_informative_feature(self,train_data, label, class_list):\n",
    "        feature_list = train_data.columns.drop(label)\n",
    "        max_info_gain = -1\n",
    "        max_info_feature = None\n",
    "\n",
    "        for feature in feature_list:  \n",
    "            feature_info_gain = self.calc_info_gain(feature, train_data, label, class_list)\n",
    "            if max_info_gain < feature_info_gain:\n",
    "                max_info_gain = feature_info_gain\n",
    "                max_info_feature = feature\n",
    "\n",
    "        return max_info_feature\n",
    "    \n",
    "    def generate_sub_tree(self,feature_name, train_data, label, class_list):\n",
    "        feature_value_count_dict = train_data[feature_name].value_counts(sort=False)\n",
    "        tree = {}\n",
    "\n",
    "        for feature_value, count in feature_value_count_dict.iteritems():\n",
    "            feature_value_data = train_data[train_data[feature_name] == feature_value]\n",
    "\n",
    "            assigned_to_node = False\n",
    "            for c in class_list:\n",
    "                class_count = feature_value_data[feature_value_data[label] == c].shape[0]\n",
    "\n",
    "                if class_count == count:\n",
    "                    tree[feature_value] = c\n",
    "                    train_data = train_data[train_data[feature_name] != feature_value]\n",
    "                    assigned_to_node = True\n",
    "            if not assigned_to_node:\n",
    "                tree[feature_value] = \"?\"\n",
    "\n",
    "        return tree, train_data\n",
    "    \n",
    "    def make_tree(self,root, prev_feature_value, train_data, label, class_list):\n",
    "        if train_data.shape[0] != 0:\n",
    "            max_info_feature = self.find_most_informative_feature(train_data, label, class_list)\n",
    "            tree, train_data = self.generate_sub_tree(max_info_feature, train_data, label, class_list)\n",
    "            next_root = None\n",
    "\n",
    "            if prev_feature_value != None:\n",
    "                root[prev_feature_value] = dict()\n",
    "                root[prev_feature_value][max_info_feature] = tree\n",
    "                next_root = root[prev_feature_value][max_info_feature]\n",
    "            else:\n",
    "                root[max_info_feature] = tree\n",
    "                next_root = root[max_info_feature]\n",
    "\n",
    "            for node, branch in list(next_root.items()):\n",
    "                if branch == \"?\":\n",
    "                    feature_value_data = train_data[train_data[max_info_feature] == node]\n",
    "                    self.make_tree(next_root, node, feature_value_data, label, class_list)\n",
    "    \n",
    "    def id3(self,train_data_m, label):\n",
    "        train_data = train_data_m.copy()\n",
    "        tree = {}\n",
    "        class_list = train_data[label].unique()\n",
    "        self.make_tree(tree, None, train_data_m, label, class_list)\n",
    "\n",
    "        return tree\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tree, instance):\n",
    "    if not isinstance(tree, dict):\n",
    "        return tree\n",
    "    else:\n",
    "        root_node = next(iter(tree))\n",
    "        feature_value = instance[root_node]\n",
    "        if feature_value in tree[root_node]:\n",
    "            return predict(tree[root_node][feature_value], instance)\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "def evaluate(tree, test_data_m, label):\n",
    "    correct_preditct = 0\n",
    "    wrong_preditct = 0\n",
    "    for index, row in test_data_m.iterrows():\n",
    "        result = predict(tree, test_data_m.loc[index])\n",
    "        if result == test_data_m[label].loc[index]:\n",
    "            correct_preditct += 1\n",
    "        else:\n",
    "            wrong_preditct += 1\n",
    "    accuracy = correct_preditct / (correct_preditct + wrong_preditct)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ada Boost Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionStump:\n",
    "    def __init__(self):\n",
    "        self.polarity = 1\n",
    "        self.threshold = None\n",
    "        self.feature_idx = None\n",
    "        self.alpha = None\n",
    "        \n",
    "    def predict(self,X):\n",
    "        n_samples = X.shape[0]\n",
    "        X_c = X[:,self.feature_idx]\n",
    "        preds = np.ones(n_samples)\n",
    "        \n",
    "        if self.polarity ==1:\n",
    "            preds[X_c < self.threshold] = -1\n",
    "        else:\n",
    "            preds[X_c > self.threshold] = -1\n",
    "            \n",
    "        return preds\n",
    "      \n",
    "class myAdaBoost:\n",
    "    def __init__(self,n_clf=5):\n",
    "        self.n_clf = n_clf\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        n_samples,n_features = X.shape\n",
    "        w = np.full(n_samples, (1/n_samples))\n",
    "        \n",
    "        self.clfs=[]\n",
    "        for _ in range(self.n_clf):\n",
    "            clf = DecisionStump()\n",
    "            min_error = float('inf')\n",
    "            for feat in range(n_features):\n",
    "                X_c = X[:,feat]\n",
    "                thresholds=np.unique(X_c)\n",
    "                for threshold in thresholds:\n",
    "                    p=1\n",
    "                    preds=np.ones(n_samples)\n",
    "                    preds[X_c<threshold]=-1\n",
    "                    \n",
    "                    misclassified = w[y!=preds]\n",
    "                    error=sum(misclassified)\n",
    "                    \n",
    "                    if error >0.5:\n",
    "                        p=-1\n",
    "                        error=1-error\n",
    "                    \n",
    "                    if error<min_error:\n",
    "                        min_error=error\n",
    "                        clf.threshold=threshold\n",
    "                        clf.feature_idx=feat\n",
    "                        clf.polarity=p\n",
    "            \n",
    "            EPS=1e-10\n",
    "            clf.alpha=0.5*np.log((1.0-min_error+EPS)/(min_error+EPS))\n",
    "            preds = clf.predict(X)\n",
    "            w *= np.exp(-clf.alpha*y*preds)\n",
    "            w/=np.sum(w)\n",
    "            self.clfs.append(clf)\n",
    "            \n",
    "    def predict(self,X):\n",
    "        clf_preds = [clf.alpha*clf.predict(X) for clf in self.clfs]\n",
    "        y_pred = np.sum(clf_preds,axis=0)\n",
    "        y_pred = np.sign(y_pred)\n",
    "        return y_pred\n",
    "\n",
    "## Method to calculate accuracy\n",
    "def accuracy(y_true, y_pred):\n",
    "    accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def entropy(y):\n",
    "    hist = np.bincount(y)\n",
    "    ps = hist / len(y)\n",
    "    return -np.sum([p * np.log2(p) for p in ps if p > 0])\n",
    "\n",
    "class Node:\n",
    "    def __init__(\n",
    "        self, feature=None, threshold=None, left=None, right=None, *, value=None\n",
    "    ):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, min_samples_split=2, max_depth=100, n_feats=None):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.n_feats = n_feats\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.n_feats = X.shape[1] if not self.n_feats else min(self.n_feats, X.shape[1])\n",
    "        self.root = self._grow_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "\n",
    "        # stopping criteria\n",
    "        if (\n",
    "            depth >= self.max_depth\n",
    "            or n_labels == 1\n",
    "            or n_samples < self.min_samples_split\n",
    "        ):\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        feat_idxs = np.random.choice(n_features, self.n_feats, replace=False)\n",
    "\n",
    "        # greedily select the best split according to information gain\n",
    "        best_feat, best_thresh = self._best_criteria(X, y, feat_idxs)\n",
    "\n",
    "        # grow the children that result from the split\n",
    "        left_idxs, right_idxs = self._split(X[:, best_feat], best_thresh)\n",
    "        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth + 1)\n",
    "        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth + 1)\n",
    "        return Node(best_feat, best_thresh, left, right)\n",
    "\n",
    "    def _best_criteria(self, X, y, feat_idxs):\n",
    "        best_gain = -1\n",
    "        split_idx, split_thresh = None, None\n",
    "        for feat_idx in feat_idxs:\n",
    "            X_column = X[:, feat_idx]\n",
    "            thresholds = np.unique(X_column)\n",
    "            for threshold in thresholds:\n",
    "                gain = self._information_gain(y, X_column, threshold)\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_idx = feat_idx\n",
    "                    split_thresh = threshold\n",
    "\n",
    "        return split_idx, split_thresh\n",
    "\n",
    "    def _information_gain(self, y, X_column, split_thresh):\n",
    "        # parent loss\n",
    "        parent_entropy = entropy(y)\n",
    "\n",
    "        # generate split\n",
    "        left_idxs, right_idxs = self._split(X_column, split_thresh)\n",
    "\n",
    "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "            return 0\n",
    "\n",
    "        # compute the weighted avg. of the loss for the children\n",
    "        n = len(y)\n",
    "        n_l, n_r = len(left_idxs), len(right_idxs)\n",
    "        e_l, e_r = entropy(y[left_idxs]), entropy(y[right_idxs])\n",
    "        child_entropy = (n_l / n) * e_l + (n_r / n) * e_r\n",
    "\n",
    "        # information gain is difference in loss before vs. after split\n",
    "        ig = parent_entropy - child_entropy\n",
    "        return ig\n",
    "\n",
    "    def _split(self, X_column, split_thresh):\n",
    "        left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n",
    "        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
    "        return left_idxs, right_idxs\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.is_leaf_node():\n",
    "            return node.value\n",
    "\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        counter = Counter(y)\n",
    "        most_common = counter.most_common(1)[0][0]\n",
    "        return most_common\n",
    "\n",
    "\n",
    "def bootstrap_sample(X, y):\n",
    "    n_samples = X.shape[0]\n",
    "    idxs = np.random.choice(n_samples, n_samples, replace=True)\n",
    "    return X[idxs], y[idxs]\n",
    "\n",
    "\n",
    "def most_common_label(y):\n",
    "    counter = Counter(y)\n",
    "    most_common = counter.most_common(1)[0][0]\n",
    "    return most_common\n",
    "\n",
    "\n",
    "class RandomForest:\n",
    "    def __init__(self, n_trees=10, min_samples_split=2, max_depth=100, n_feats=None):\n",
    "        self.n_trees = n_trees\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.n_feats = n_feats\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.trees = []\n",
    "        for _ in range(self.n_trees):\n",
    "            tree = DecisionTree(\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                max_depth=self.max_depth,\n",
    "                n_feats=self.n_feats,\n",
    "            )\n",
    "            X_samp, y_samp = bootstrap_sample(X, y)\n",
    "            tree.fit(X_samp, y_samp)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
    "        tree_preds = np.swapaxes(tree_preds, 0, 1)\n",
    "        y_pred = [most_common_label(tree_pred) for tree_pred in tree_preds]\n",
    "        return np.array(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ID3:\n",
    "    def __init__(self):\n",
    "        self.tree={}\n",
    "        \n",
    "    def calc_entropy(self,feature_data, label, class_list):\n",
    "        class_count = feature_data.shape[0]\n",
    "        entropy = 0\n",
    "\n",
    "        for c in class_list:\n",
    "            label_class_count = feature_data[feature_data[label] == c].shape[0]\n",
    "\n",
    "            entropy_class = 0\n",
    "            if label_class_count != 0:\n",
    "                probability_class = label_class_count/class_count\n",
    "                entropy_class = - probability_class * np.log2(probability_class) \n",
    "\n",
    "            entropy += entropy_class\n",
    "\n",
    "        return entropy\n",
    "        \n",
    "    def total_entropy(self,train_data, label, class_list):\n",
    "        total_row = train_data.shape[0]\n",
    "        total_entr = 0\n",
    "\n",
    "        for c in class_list:\n",
    "            total_class_count = train_data[train_data[label] == c].shape[0]\n",
    "            total_class_entr = - (total_class_count/total_row)*np.log2(total_class_count/total_row) \n",
    "            total_entr += total_class_entr\n",
    "\n",
    "        return total_entr\n",
    "    \n",
    "    \n",
    "    def info_gain(self,feature_name, train_data, label, class_list):\n",
    "        flist = train_data[feature_name].unique()\n",
    "        total_row = train_data.shape[0]\n",
    "        feature_info = 0.0\n",
    "\n",
    "        for feature_value in flist:\n",
    "            fdata = train_data[train_data[feature_name] == feature_value]\n",
    "            feature_count = fdata.shape[0]\n",
    "            feature_value_entropy = self.calc_entropy(fdata, label, class_list)\n",
    "            feature_value_probability = feature_count/total_row\n",
    "            feature_info += feature_value_probability * feature_value_entropy\n",
    "\n",
    "        return self.total_entropy(train_data, label, class_list) - feature_info\n",
    "    \n",
    "    def most_informative_feature(self,train_data, label, class_list):\n",
    "        feature_list = train_data.columns.drop(label)\n",
    "        max_info = -1\n",
    "        maxfeature = None\n",
    "\n",
    "        for feature in feature_list:  \n",
    "            feature_info_gain = self.info_gain(feature, train_data, label, class_list)\n",
    "            if max_info < feature_info_gain:\n",
    "                max_info = feature_info_gain\n",
    "                maxfeature = feature\n",
    "\n",
    "        return maxfeature\n",
    "    \n",
    "    def sub_tree(self,fname, train_data, label, class_list):\n",
    "        feature_value_count_dict = train_data[fname].value_counts(sort=False)\n",
    "        tree = {}\n",
    "\n",
    "        for fvalue, count in feature_value_count_dict.iteritems():\n",
    "            feature_value_data = train_data[train_data[fname] == fvalue]\n",
    "\n",
    "            assigned = False\n",
    "            for c in class_list:\n",
    "                ccount = feature_value_data[feature_value_data[label] == c].shape[0]\n",
    "                if ccount == count:\n",
    "                    tree[fvalue] = c\n",
    "                    train_data = train_data[train_data[fname] != fvalue]\n",
    "                    assigned = True\n",
    "            if not assigned:\n",
    "                tree[fvalue] = \"?\"\n",
    "\n",
    "        return tree, train_data\n",
    "    \n",
    "    def buildtree(self,root, prev_feature, train_data, label, cls):\n",
    "        if train_data.shape[0] != 0:\n",
    "            max_info_f = self.most_informative_feature(train_data, label, cls)\n",
    "            tree, train_data = self.sub_tree(max_info_f, train_data, label, cls)\n",
    "            next_root = None\n",
    "\n",
    "            if prev_feature != None:\n",
    "                root[prev_feature] = dict()\n",
    "                root[prev_feature][max_info_f] = tree\n",
    "                next_root = root[prev_feature][max_info_f]\n",
    "            else:\n",
    "                root[max_info_f] = tree\n",
    "                next_root = root[max_info_f]\n",
    "\n",
    "            for node, branch in list(next_root.items()):\n",
    "                if branch == \"?\":\n",
    "                    feature_value = train_data[train_data[max_info_f] == node]\n",
    "                    self.buildtree(next_root, node, feature_value, label, cls)\n",
    "    \n",
    "    def id3(self,train_data_m, label):\n",
    "        train_data = train_data_m.copy()\n",
    "        tree = {}\n",
    "        class_list = train_data[label].unique()\n",
    "        self.buildtree(tree, None, train_data_m, label, class_list)\n",
    "\n",
    "        return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self._classes = np.unique(y)\n",
    "        n_classes = len(self._classes)\n",
    "\n",
    "        # calculate mean, var, and prior for each class\n",
    "        self._mean = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "        self._var = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "        self._priors = np.zeros(n_classes, dtype=np.float64)\n",
    "\n",
    "        for idx, c in enumerate(self._classes):\n",
    "            X_c = X[y == c]\n",
    "            self._mean[idx, :] = X_c.mean(axis=0)\n",
    "            self._var[idx, :] = X_c.var(axis=0)\n",
    "            self._priors[idx] = X_c.shape[0] / float(n_samples)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = [self._predict(x) for x in X]\n",
    "        return np.array(y_pred)\n",
    "\n",
    "    def _predict(self, x):\n",
    "        posteriors = []\n",
    "\n",
    "        # calculate posterior probability for each class\n",
    "        for idx, c in enumerate(self._classes):\n",
    "            prior = np.log(self._priors[idx])\n",
    "            posterior = np.sum(np.log(self._pdf(idx, x)))\n",
    "            posterior = prior + posterior\n",
    "            posteriors.append(posterior)\n",
    "\n",
    "        # return class with highest posterior probability\n",
    "        return self._classes[np.argmax(posteriors)]\n",
    "\n",
    "    def _pdf(self, class_idx, x):\n",
    "        mean = self._mean[class_idx]\n",
    "        var = self._var[class_idx]\n",
    "        numerator = np.exp(-((x - mean) ** 2) / (2 * var))\n",
    "        denominator = np.sqrt(2 * np.pi * var)\n",
    "        return numerator / denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Distance 3 Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def euclidean_distance(x1, x2):\n",
    "    return np.sqrt(np.sum(x1 -x2) ** 2)\n",
    "\n",
    "\n",
    "class KNN:\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = [self._predict(x) for x in X]\n",
    "        return np.array(y_pred)\n",
    "\n",
    "    def _predict(self, x):\n",
    "        # Compute distances between x and all examples in the training set\n",
    "        distances = [euclidean_distance(x, x_train) for x_train in self.X_train]\n",
    "        # Sort by distance and return indices of the first k neighbors\n",
    "        k_idx = np.argsort(distances)[: self.k]\n",
    "        # Extract the labels of the k nearest neighbor training samples\n",
    "        k_neighbor_labels = [self.y_train[i] for i in k_idx]\n",
    "        # return the most common class label\n",
    "        most_common = Counter(k_neighbor_labels).most_common(1)\n",
    "        return most_common[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self._classes = np.unique(y)\n",
    "        n_classes = len(self._classes)\n",
    "\n",
    "        self.mean_val = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "        self.var_val = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "        self.priors = np.zeros(n_classes, dtype=np.float64)\n",
    "\n",
    "        for idx, c in enumerate(self._classes):\n",
    "            X_change = X[y == c]\n",
    "            self.mean_val[idx, :] = X_change.mean(axis=0)\n",
    "            self.var_val[idx, :] = X_change.var(axis=0)\n",
    "            self.priors[idx] = X_change.shape[0] / float(n_samples)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = [self._predict(x) for x in X]\n",
    "        return y_pred\n",
    "\n",
    "    def _predict(self, x):\n",
    "        posteriors = []\n",
    "\n",
    "        for idx, c in enumerate(self._classes):\n",
    "            prior = np.log(self.priors[idx])\n",
    "            posterior = np.sum(np.log(self.pdf(idx, x)))\n",
    "            posterior = prior + posterior\n",
    "            posteriors.append(posterior)\n",
    "\n",
    "        return self._classes[np.argmax(posteriors)]\n",
    "\n",
    "    def pdf(self, class_idx, x):\n",
    "        mean = self.mean_val[class_idx]\n",
    "        var = self.var_val[class_idx]\n",
    "        numerator = np.exp(-((x - mean) ** 2) / (2 * var))\n",
    "        denominator = np.sqrt(2 * np.pi * var)\n",
    "        return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breast Cancer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances = 699\n",
      "Number of attributes = 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clump Thickness</th>\n",
       "      <th>Uniformity of Cell Size</th>\n",
       "      <th>Uniformity of Cell Shape</th>\n",
       "      <th>Marginal Adhesion</th>\n",
       "      <th>Single Epithelial Cell Size</th>\n",
       "      <th>Bare Nuclei</th>\n",
       "      <th>Bland Chromatin</th>\n",
       "      <th>Normal Nucleoli</th>\n",
       "      <th>Mitoses</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Clump Thickness  Uniformity of Cell Size  Uniformity of Cell Shape  \\\n",
       "0                5                        1                         1   \n",
       "1                5                        4                         4   \n",
       "2                3                        1                         1   \n",
       "3                6                        8                         8   \n",
       "4                4                        1                         1   \n",
       "\n",
       "   Marginal Adhesion  Single Epithelial Cell Size Bare Nuclei  \\\n",
       "0                  1                            2           1   \n",
       "1                  5                            7          10   \n",
       "2                  1                            2           2   \n",
       "3                  1                            3           4   \n",
       "4                  3                            2           1   \n",
       "\n",
       "   Bland Chromatin  Normal Nucleoli  Mitoses  Class  \n",
       "0                3                1        1      2  \n",
       "1                3                2        1      2  \n",
       "2                3                1        1      2  \n",
       "3                3                7        1      2  \n",
       "4                3                1        1      2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('breast-cancer-wisconsin.data', header=None)\n",
    "data.columns = ['Sample code', 'Clump Thickness', 'Uniformity of Cell Size', 'Uniformity of Cell Shape',\n",
    "                'Marginal Adhesion', 'Single Epithelial Cell Size', 'Bare Nuclei', 'Bland Chromatin',\n",
    "                'Normal Nucleoli', 'Mitoses','Class']\n",
    "\n",
    "data = data.drop(['Sample code'],axis=1)\n",
    "print('Number of instances = %d' % (data.shape[0]))\n",
    "print('Number of attributes = %d' % (data.shape[1]))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances = 699\n",
      "Number of attributes = 10\n",
      "Number of missing values:\n",
      "\tClump Thickness: 0\n",
      "\tUniformity of Cell Size: 0\n",
      "\tUniformity of Cell Shape: 0\n",
      "\tMarginal Adhesion: 0\n",
      "\tSingle Epithelial Cell Size: 0\n",
      "\tBare Nuclei: 16\n",
      "\tBland Chromatin: 0\n",
      "\tNormal Nucleoli: 0\n",
      "\tMitoses: 0\n",
      "\tClass: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = data.replace('?',np.NaN)\n",
    "\n",
    "print('Number of instances = %d' % (data.shape[0]))\n",
    "print('Number of attributes = %d' % (data.shape[1]))\n",
    "\n",
    "print('Number of missing values:')\n",
    "for col in data.columns:\n",
    "    print('\\t%s: %d' % (col,data[col].isna().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before replacing missing values:\n",
      "20     10\n",
      "21      7\n",
      "22      1\n",
      "23    NaN\n",
      "24      1\n",
      "Name: Bare Nuclei, dtype: object\n",
      "\n",
      "After replacing missing values:\n",
      "20    10\n",
      "21     7\n",
      "22     1\n",
      "23     1\n",
      "24     1\n",
      "Name: Bare Nuclei, dtype: object\n"
     ]
    }
   ],
   "source": [
    "data2 = data['Bare Nuclei']\n",
    "\n",
    "print('Before replacing missing values:')\n",
    "print(data2[20:25])\n",
    "data2 = data2.fillna(data2.median())\n",
    "\n",
    "print('\\nAfter replacing missing values:')\n",
    "print(data2[20:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in original data = 699\n",
      "Number of rows after discarding missing values = 683\n"
     ]
    }
   ],
   "source": [
    "print('Number of rows in original data = %d' % (data.shape[0]))\n",
    "\n",
    "data2 = data.dropna()\n",
    "print('Number of rows after discarding missing values = %d' % (data2.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((683, 9), (683,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = data2.iloc[:,:-1].values.astype(int)\n",
    "\n",
    "y = data2.iloc[:,-1].values.astype(int)\n",
    "x.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedKFold\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=10, random_state=None)\n",
    "cnt=0\n",
    "r=0\n",
    "# X is the feature set and y is the target\n",
    "for train_index, test_index in cv.split(x,y):\n",
    "       \n",
    "    #print(\"Train:\", train_index, \"Validation:\", test_index)\n",
    "    X_train, X_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    clf = RandomForest(n_trees=3, max_depth=10)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc = accuracy(y_test, y_pred)\n",
    "\n",
    "    print(\"Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.95%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RepeatedKFold\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=10, random_state=None)\n",
    "cnt=0\n",
    "r=0\n",
    "# X is the feature set and y is the target\n",
    "for train_index, test_index in cv.split(x,y):\n",
    "    scores=[]\n",
    "     \n",
    "    #print(\"Train:\", train_index, \"Validation:\", test_index)\n",
    "    X_train, X_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    y_train = pd.DataFrame(y_train)\n",
    "    y_test = pd.DataFrame(y_test)\n",
    "    x_test = pd.DataFrame(X_test)\n",
    "    X_train['Class']=y_train\n",
    "    train=X_train\n",
    "    x_test['Class']=y_test\n",
    "    test=x_test\n",
    "    t=ID3()\n",
    "    tree=t.id3(train,'Class')\n",
    "    accuracy = evaluate(tree, test, 'Class')\n",
    "    scores.append(accuracy)\n",
    "print('Accuracy: %.2f%%' % (sum(scores)/float(len(scores))))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files (x86)\\microsoft visual studio\\shared\\python37_64\\lib\\site-packages\\ipykernel_launcher.py:26: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.98%\n"
     ]
    }
   ],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=10, random_state=None)\n",
    "cnt=0\n",
    "r=0\n",
    "# X is the feature set and y is the target\n",
    "for train_index, test_index in cv.split(x,y):\n",
    "    scores=[]   \n",
    "    #print(\"Train:\", train_index, \"Validation:\", test_index)\n",
    "    X_train, X_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    nb = NaiveBayes()\n",
    "    nb.fit(X_train, y_train)\n",
    "    predictions = nb.predict(X_test)\n",
    "    score=accuracy(y_test, predictions)\n",
    "    scores.append(score)\n",
    "print('Accuracy: %.2f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN DISTANCE 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=10, random_state=None)\n",
    "cnt=0\n",
    "r=0\n",
    "# X is the feature set and y is the target\n",
    "for train_index, test_index in cv.split(x,y):\n",
    "    scores=[]\n",
    "    #print(\"Train:\", train_index, \"Validation:\", test_index)\n",
    "    X_train, X_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    k = 3\n",
    "    clf = KNN(k=k)\n",
    "    clf.fit(X_train, y_train)\n",
    "    predictions = clf.predict(X_test)\n",
    "    score=accuracy(y_test, predictions)\n",
    "    scores.append(score)\n",
    "print('Accuracy: %.2f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADA BOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "    return accuracy\n",
    "\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=10, random_state=None)\n",
    "cnt=0\n",
    "r=0\n",
    "y[y==2]=1\n",
    "y[y==4]=-1\n",
    "# X is the feature set and y is the target\n",
    "for train_index, test_index in cv.split(x,y):\n",
    "    scores=[]  \n",
    "    #print(\"Train:\", train_index, \"Validation:\", test_index)\n",
    "    X_train, X_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    clf = myAdaBoost()\n",
    "    ## Fitting the model on train data\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    ## Predict the target's for 20% test data\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc = accuracy(y_test, y_pred)\n",
    "    scores.append(acc)\n",
    "print('Accuracy: %.2f%%' % (sum(scores)/float(len(scores))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Car Datasetr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('car.data', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'class']\n",
    "\n",
    "\n",
    "df.columns = col_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['class'], axis=1)\n",
    "\n",
    "y = df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary package for encoding our categorial features\n",
    "import category_encoders as ce\n",
    "\n",
    "encoder_X = ce.OrdinalEncoder(cols=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety'])\n",
    "x= encoder_X.fit_transform(X)\n",
    "\n",
    "encoder_Y = ce.OrdinalEncoder()\n",
    "y=np.ravel(encoder_Y.fit_transform(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pd.DataFrame(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=pd.DataFrame(y)\n",
    "y.columns=['class']\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.values.astype(int)\n",
    "\n",
    "y = y.values.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedKFold\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=10, random_state=None)\n",
    "cnt=0\n",
    "r=0\n",
    "\n",
    "y[y==1]=2\n",
    "y[y==2]=4\n",
    "y[y==3]=6\n",
    "y[y==4]=8\n",
    "# X is the feature set and y is the target\n",
    "for train_index, test_index in cv.split(x,y):\n",
    "    \n",
    "    scores=[]\n",
    "    #print(\"Train:\", train_index, \"Validation:\", test_index)\n",
    "    X_train, X_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    y_train = pd.DataFrame(y_train)\n",
    "    y_test = pd.DataFrame(y_test)\n",
    "    x_test = pd.DataFrame(X_test)\n",
    "    X_train['class']=y_train\n",
    "    train=X_train\n",
    "    x_test['class']=y_test\n",
    "    test=x_test\n",
    "    t=ID3()\n",
    "    tree=t.id3(train,'class')\n",
    "    score = evaluate(tree, test, 'class')\n",
    "#     score=accuracy(y_test, test_predictions)\n",
    "    scores.append(score)\n",
    "print('Accuracy: %.2f%%' % (sum(scores)/float(len(scores))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedKFold\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=10, random_state=None)\n",
    "cnt=0\n",
    "r=0\n",
    "# X is the feature set and y is the target\n",
    "for train_index, test_index in cv.split(x,y):\n",
    "       \n",
    "    #print(\"Train:\", train_index, \"Validation:\", test_index)\n",
    "    X_train, X_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    clf = myAdaBoost()\n",
    "    ## Fitting the model on train data\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "#     ## Predict the target's for 20% test data\n",
    "#     y_pred = clf.predict(X_test)\n",
    "#     acc = accuracy(y_test, y_pred)\n",
    "#     print('accuracy: ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=10, random_state=None)\n",
    "cnt=0\n",
    "r=0\n",
    "# X is the feature set and y is the target\n",
    "for train_index, test_index in cv.split(x,y):\n",
    "       \n",
    "    #print(\"Train:\", train_index, \"Validation:\", test_index)\n",
    "    X_train, X_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    k = 3\n",
    "    clf = KNN(k=k)\n",
    "    clf.fit(X_train, y_train)\n",
    "    predictions = clf.predict(X_test)\n",
    "    print(\"KNN classification accuracy\", accuracy(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mushroom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df =  pd.read_csv(\"mushroom.data\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['class', 'cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor',\n",
    "       'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color',\n",
    "       'stalk-shape', 'stalk-root', 'stalk-surface-above-ring',\n",
    "       'stalk-surface-below-ring', 'stalk-color-above-ring',\n",
    "       'stalk-color-below-ring', 'veil-type', 'veil-color', 'ring-number',\n",
    "       'ring-type', 'spore-print-color', 'population', 'habitat']\n",
    "col=['class', 'cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor',\n",
    "       'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color',\n",
    "       'stalk-shape', 'stalk-root', 'stalk-surface-above-ring',\n",
    "       'stalk-surface-below-ring', 'stalk-color-above-ring',\n",
    "       'stalk-color-below-ring', 'veil-type', 'veil-color', 'ring-number',\n",
    "       'ring-type', 'spore-print-color', 'population', 'habitat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder=LabelEncoder()\n",
    "for column in df.columns:\n",
    "    df[column] = labelencoder.fit_transform(df[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in col:\n",
    "    print(len(df[column].unique())-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in col:\n",
    "\n",
    "    fi=(len(df[column].unique())-1)\n",
    "    value=0\n",
    "    step = 1/(fi+1)\n",
    "    for i in df[column].unique():\n",
    "        df[column] = [value if letter == i else letter for letter in df[column]]\n",
    "        value += step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.iloc[:,1:].values.astype(int)\n",
    "y= df.iloc[:,:1].values.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedKFold\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=10, random_state=None)\n",
    "cnt=0\n",
    "r=0\n",
    "# X is the feature set and y is the target\n",
    "for train_index, test_index in cv.split(x,y):\n",
    "       \n",
    "    #print(\"Train:\", train_index, \"Validation:\", test_index)\n",
    "    X_train, X_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    clf = RandomForest(n_trees=5, max_depth=10)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc = accuracy(y_test, y_pred)\n",
    "\n",
    "    print(\"Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedKFold\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=10, random_state=None)\n",
    "cnt=0\n",
    "r=0\n",
    "\n",
    "y[y==1]=2\n",
    "y[y==2]=4\n",
    "# X is the feature set and y is the target\n",
    "for train_index, test_index in cv.split(x,y):\n",
    "    \n",
    "     \n",
    "    #print(\"Train:\", train_index, \"Validation:\", test_index)\n",
    "    X_train, X_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    y_train = pd.DataFrame(y_train)\n",
    "    y_test = pd.DataFrame(y_test)\n",
    "    x_test = pd.DataFrame(X_test)\n",
    "    X_train['Class']=y_train\n",
    "    train=X_train\n",
    "    x_test['Class']=y_test\n",
    "    test=x_test\n",
    "    t=ID3()\n",
    "    tree=t.id3(train,'Class')\n",
    "    accuracy = evaluate(tree, test, 'Class')\n",
    "    print('accuracy: '+ str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=10, random_state=None)\n",
    "cnt=0\n",
    "r=0\n",
    "# X is the feature set and y is the target\n",
    "for train_index, test_index in cv.split(x,y):\n",
    "       \n",
    "    #print(\"Train:\", train_index, \"Validation:\", test_index)\n",
    "    X_train, X_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    nb = NaiveBayes()\n",
    "    nb.fit(X_train, y_train)\n",
    "    predictions = nb.predict(X_test)\n",
    "\n",
    "    print(\"Naive Bayes classification accuracy\", accuracy(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=10, random_state=None)\n",
    "cnt=0\n",
    "r=0\n",
    "# X is the feature set and y is the target\n",
    "for train_index, test_index in cv.split(x,y):\n",
    "       \n",
    "    #print(\"Train:\", train_index, \"Validation:\", test_index)\n",
    "    X_train, X_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    k = 5\n",
    "    clf = KNN(k=k)\n",
    "    clf.fit(X_train, y_train)\n",
    "    predictions = clf.predict(X_test)\n",
    "    print(\"KNN classification accuracy\", accuracy(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedKFold\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=10, random_state=None)\n",
    "cnt=0\n",
    "r=0\n",
    "\n",
    "y[y==1]=1\n",
    "y[y==2]=-1\n",
    "# X is the feature set and y is the target\n",
    "for train_index, test_index in cv.split(x,y):\n",
    "       \n",
    "    #print(\"Train:\", train_index, \"Validation:\", test_index)\n",
    "    X_train, X_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    clf = myAdaBoost()\n",
    "    ## Fitting the model on train data\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    ## Predict the target's for 20% test data\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc = accuracy(y_test, y_pred)\n",
    "    print('accuracy: ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.core.umath_tests import inner1d\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "class AdaBoostClassifier(object):\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        if kwargs and args:\n",
    "            raise ValueError(\n",
    "                '''AdaBoostClassifier can only be called with keyword\n",
    "                   arguments for the following keywords: base_estimator ,n_estimators,\n",
    "                    learning_rate,algorithm,random_state''')\n",
    "        allowed_keys = ['base_estimator', 'n_estimators', 'learning_rate', 'algorithm', 'random_state']\n",
    "        keywords_used = kwargs.keys()\n",
    "        for keyword in keywords_used:\n",
    "            if keyword not in allowed_keys:\n",
    "                raise ValueError(keyword + \":  Wrong keyword used --- check spelling\")\n",
    "\n",
    "        n_estimators = 50\n",
    "        learning_rate = 1\n",
    "        algorithm = 'SAMME.R'\n",
    "        random_state = None\n",
    "\n",
    "        if kwargs and not args:\n",
    "            if 'base_estimator' in kwargs:\n",
    "                base_estimator = kwargs.pop('base_estimator')\n",
    "            else:\n",
    "                raise ValueError('''base_estimator can not be None''')\n",
    "            if 'n_estimators' in kwargs: n_estimators = kwargs.pop('n_estimators')\n",
    "            if 'learning_rate' in kwargs: learning_rate = kwargs.pop('learning_rate')\n",
    "            if 'algorithm' in kwargs: algorithm = kwargs.pop('algorithm')\n",
    "            if 'random_state' in kwargs: random_state = kwargs.pop('random_state')\n",
    "\n",
    "        self.base_estimator_ = base_estimator\n",
    "        self.n_estimators_ = n_estimators\n",
    "        self.learning_rate_ = learning_rate\n",
    "        self.algorithm_ = algorithm\n",
    "        self.random_state_ = random_state\n",
    "        self.estimators_ = list()\n",
    "        self.estimator_weights_ = np.zeros(self.n_estimators_)\n",
    "        self.estimator_errors_ = np.ones(self.n_estimators_)\n",
    "\n",
    "\n",
    "    def _samme_proba(self, estimator, n_classes, X):\n",
    "        \"\"\"Calculate algorithm 4, step 2, equation c) of Zhu et al [1].\n",
    "        References\n",
    "        ----------\n",
    "        .. [1] J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class AdaBoost\", 2009.\n",
    "        \"\"\"\n",
    "        proba = estimator.predict_proba(X)\n",
    "\n",
    "        # Displace zero probabilities so the log is defined.\n",
    "        # Also fix negative elements which may occur with\n",
    "        # negative sample weights.\n",
    "        proba[proba < np.finfo(proba.dtype).eps] = np.finfo(proba.dtype).eps\n",
    "        log_proba = np.log(proba)\n",
    "\n",
    "        return (n_classes - 1) * (log_proba - (1. / n_classes)\n",
    "                                  * log_proba.sum(axis=1)[:, np.newaxis])\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.n_samples = X.shape[0]\n",
    "        # There is hidden trouble for classes, here the classes will be sorted.\n",
    "        # So in boost we have to ensure that the predict results have the same classes sort\n",
    "        self.classes_ = np.array(sorted(list(set(y))))\n",
    "        self.n_classes_ = len(self.classes_)\n",
    "        for iboost in range(self.n_estimators_):\n",
    "            if iboost == 0:\n",
    "                sample_weight = np.ones(self.n_samples) / self.n_samples\n",
    "\n",
    "            sample_weight, estimator_weight, estimator_error = self.boost(X, y, sample_weight)\n",
    "\n",
    "            # early stop\n",
    "            if estimator_error == None:\n",
    "                break\n",
    "\n",
    "            # append error and weight\n",
    "            self.estimator_errors_[iboost] = estimator_error\n",
    "            self.estimator_weights_[iboost] = estimator_weight\n",
    "\n",
    "            if estimator_error <= 0:\n",
    "                break\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    def boost(self, X, y, sample_weight):\n",
    "        if self.algorithm_ == 'SAMME':\n",
    "            return self.discrete_boost(X, y, sample_weight)\n",
    "        elif self.algorithm_ == 'SAMME.R':\n",
    "            return self.real_boost(X, y, sample_weight)\n",
    "\n",
    "    def real_boost(self, X, y, sample_weight):\n",
    "        estimator = deepcopy(self.base_estimator_)\n",
    "        if self.random_state_:\n",
    "            estimator.set_params(random_state=1)\n",
    "\n",
    "        estimator.fit(X, y, sample_weight=sample_weight)\n",
    "\n",
    "        y_pred = estimator.predict(X)\n",
    "        incorrect = y_pred != y\n",
    "        estimator_error = np.dot(incorrect, sample_weight) / np.sum(sample_weight, axis=0)\n",
    "\n",
    "        # if worse than random guess, stop boosting\n",
    "        if estimator_error >= 1.0 - 1 / self.n_classes_:\n",
    "            return None, None, None\n",
    "\n",
    "        y_predict_proba = estimator.predict_proba(X)\n",
    "        # repalce zero\n",
    "        y_predict_proba[y_predict_proba < np.finfo(y_predict_proba.dtype).eps] = np.finfo(y_predict_proba.dtype).eps\n",
    "\n",
    "        y_codes = np.array([-1. / (self.n_classes_ - 1), 1.])\n",
    "        y_coding = y_codes.take(self.classes_ == y[:, np.newaxis])\n",
    "\n",
    "        # for sample weight update\n",
    "        intermediate_variable = (-1. * self.learning_rate_ * (((self.n_classes_ - 1) / self.n_classes_) *\n",
    "                                                              inner1d(y_coding, np.log(\n",
    "                                                                  y_predict_proba))))  #dot iterate for each row\n",
    "\n",
    "        # update sample weight\n",
    "        sample_weight *= np.exp(intermediate_variable)\n",
    "\n",
    "        sample_weight_sum = np.sum(sample_weight, axis=0)\n",
    "        if sample_weight_sum <= 0:\n",
    "            return None, None, None\n",
    "\n",
    "        # normalize sample weight\n",
    "        sample_weight /= sample_weight_sum\n",
    "\n",
    "        # append the estimator\n",
    "        self.estimators_.append(estimator)\n",
    "\n",
    "        return sample_weight, 1, estimator_error\n",
    "\n",
    "\n",
    "    def discrete_boost(self, X, y, sample_weight):\n",
    "        estimator = deepcopy(self.base_estimator_)\n",
    "        if self.random_state_:\n",
    "            estimator.set_params(random_state=1)\n",
    "\n",
    "        estimator.fit(X, y, sample_weight=sample_weight)\n",
    "\n",
    "        y_pred = estimator.predict(X)\n",
    "        incorrect = y_pred != y\n",
    "        estimator_error = np.dot(incorrect, sample_weight) / np.sum(sample_weight, axis=0)\n",
    "\n",
    "        # if worse than random guess, stop boosting\n",
    "        if estimator_error >= 1 - 1 / self.n_classes_:\n",
    "            return None, None, None\n",
    "\n",
    "        # update estimator_weight\n",
    "        estimator_weight = self.learning_rate_ * np.log((1 - estimator_error) / estimator_error) + np.log(\n",
    "            self.n_classes_ - 1)\n",
    "\n",
    "        if estimator_weight <= 0:\n",
    "            return None, None, None\n",
    "\n",
    "        # update sample weight\n",
    "        sample_weight *= np.exp(estimator_weight * incorrect)\n",
    "\n",
    "        sample_weight_sum = np.sum(sample_weight, axis=0)\n",
    "        if sample_weight_sum <= 0:\n",
    "            return None, None, None\n",
    "\n",
    "        # normalize sample weight\n",
    "        sample_weight /= sample_weight_sum\n",
    "\n",
    "        # append the estimator\n",
    "        self.estimators_.append(estimator)\n",
    "\n",
    "        return sample_weight, estimator_weight, estimator_error\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_classes = self.n_classes_\n",
    "        classes = self.classes_[:, np.newaxis]\n",
    "        pred = None\n",
    "\n",
    "        if self.algorithm_ == 'SAMME.R':\n",
    "            # The weights are all 1. for SAMME.R\n",
    "            pred = sum(self._samme_proba(estimator, n_classes, X) for estimator in self.estimators_)\n",
    "        else:  # self.algorithm == \"SAMME\"\n",
    "            pred = sum((estimator.predict(X) == classes).T * w\n",
    "                       for estimator, w in zip(self.estimators_,\n",
    "                                               self.estimator_weights_))\n",
    "\n",
    "        pred /= self.estimator_weights_.sum()\n",
    "        if n_classes == 2:\n",
    "            pred[:, 0] *= -1\n",
    "            pred = pred.sum(axis=1)\n",
    "            return self.classes_.take(pred > 0, axis=0)\n",
    "\n",
    "        return self.classes_.take(np.argmax(pred, axis=1), axis=0)\n",
    "\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if self.algorithm_ == 'SAMME.R':\n",
    "            # The weights are all 1. for SAMME.R\n",
    "            proba = sum(self._samme_proba(estimator, self.n_classes_, X)\n",
    "                        for estimator in self.estimators_)\n",
    "        else:  # self.algorithm == \"SAMME\"\n",
    "            proba = sum(estimator.predict_proba(X) * w\n",
    "                        for estimator, w in zip(self.estimators_,\n",
    "                                                self.estimator_weights_))\n",
    "\n",
    "        proba /= self.estimator_weights_.sum()\n",
    "        proba = np.exp((1. / (n_classes - 1)) * proba)\n",
    "        normalizer = proba.sum(axis=1)[:, np.newaxis]\n",
    "        normalizer[normalizer == 0.0] = 1.0\n",
    "        proba /= normalizer\n",
    "\n",
    "        return proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
